# -*- coding: utf-8 -*-
"""comparing with other optimizers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAedCD52XgTmHGBRM9_bThgvPAPr3fEx

**bold text**# Sentiment Analysis

This notebook trains a sentiment analysis model to classify movie reviews as positive or negative, based on the text of the review. This is an example of binary—or two-class—classification, an important and widely applicable kind of machine learning problem.

We'll use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.

## Setup
"""

# !pip install git+https://github.com/yunshu67/Newton-CG.git
# !pip install -i https://test.pypi.org/simple/ newton-cg==0.0.3

# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)

import numpy as np
from tensorflow import keras
import newton_cg as es
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, Bidirectional
from tensorflow.keras import optimizers

# from google.colab import drive
# drive.mount('/content/drive')
# % cd /content/drive/MyDrive/pretrained_word_embedding/
# % ls

"""## preprocess the dataset"""

#importing libraries
import pandas as pd
import numpy as np

#reading csv files
train = pd.read_csv('../data/Train.csv')
valid = pd.read_csv('../data/Valid.csv')             

#train_test split
x_tr, y_tr = train['text'].values, train['label'].values
x_val, y_val = valid['text'].values, valid['label'].values

print(type(x_tr))
print(type(x_tr[0]))
print(x_tr[0])
print(len(x_tr[0]))
print(y_tr[0])

import unicodedata
import re

## Step 1 and Step 2 
def preprocess_sentence(w):
    # w = unicode_to_ascii(w.lower().strip())
    w = w.lower().strip()

    # creating a space between a word and the punctuation following it
    # eg: "he is a boy." => "he is a boy ."
    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
    w = re.sub(r"([?.!,¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)

    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
    w = re.sub(r"[^a-zA-Záéíóúâêôãõàèìò?.!,¿]+", " ", w)

    w = w.strip()

    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    # w = '<start> ' + w + ' <end>'
    return w

x_tr = np.array([preprocess_sentence(i) for i in x_tr])
x_val = np.array([preprocess_sentence(i) for i in x_val])

print(type(x_tr))
print(type(x_tr[0]))
print(x_tr[0])
print(len(x_tr[0]))

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Tokenize the sentences
tokenizer = Tokenizer()

#preparing vocabulary
tokenizer.fit_on_texts(list(x_tr))

#converting text into integer sequences
x_tr_seq  = tokenizer.texts_to_sequences(x_tr) 
x_val_seq = tokenizer.texts_to_sequences(x_val)

#padding to prepare sequences of same length
x_tr_seq  = pad_sequences(x_tr_seq, maxlen=256)
x_val_seq = pad_sequences(x_val_seq, maxlen=256)

print(x_tr_seq[:3])
print(len(x_tr_seq))

size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding
print(size_of_vocabulary)

"""## load pretrained word embedding vectors"""

embeddings_dict = dict()
f = open('../data/cc.en.300.vec')
lines = f.readlines()

from pprint import pprint
import numpy as np
for line in lines[1:]:
    values = line.split()
    word = values[0]
    vec = np.asarray(values[1:], dtype='float32')
    embeddings_dict[word] = vec

f.close()
print('Loaded %s word vectors.' % len(embeddings_dict))
pprint(embeddings_dict['hello'])

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((size_of_vocabulary, 300))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_dict.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

"""## Model"""

tf.keras.backend.clear_session() # clear previous model

vocabulary = size_of_vocabulary
word_num = 256
state_dim = 32

model = Sequential()
# model.add(Embedding(vocabulary,embedding_dim,input_length=word_num))
model.add(Embedding(size_of_vocabulary,300,weights=[embedding_matrix],input_length=256,trainable=False)) 
model.add(SimpleRNN(state_dim, return_sequences=False, unroll=True))
# model.add(Bidirectional(SimpleRNN(state_dim, return_sequences=False, unroll=True)))
model.add(Dense(1,activation='sigmoid'))

model_name = 'bidirectional_rnn'

model.summary()

from tensorflow.keras.callbacks import EarlyStopping

cb = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)

"""### set up optimizers"""

def clear_model():
   # kill previous model
  tf.keras.backend.clear_session()
  # tf.reset_default_graph()

# Set solvers and optimizers
solvers = ['Newton-CG','Adam', 'SGD','RMSProp','AdaGrad']

# Set solvers_dicts for plot loss and acc
solvers_dicts ={key:{} for key in solvers}
print(solvers_dicts) 
train_log_keys = ['loss', 'acc', 'val_loss', 'val_acc']
for solver, _ in solvers_dicts.items():
  solvers_dicts[solver] = {key:{} for key in train_log_keys}
print(solvers_dicts)

clear_model()

# creat optimizers list
opt_adam = tf.keras.optimizers.Adam(learning_rate=0.0001)
opt_sgd = tf.keras.optimizers.SGD(learning_rate=0.001,momentum=0.9)
opt_newton_cg = es.EHNewtonOptimizer(learning_rate=0.01, tau=1000)
opt_rmsprop = tf.keras.optimizers.RMSprop(0.0001)
opt_adagrad = tf.keras.optimizers.Adagrad(0.01)
opts = [opt_newton_cg,opt_adam, opt_sgd,opt_rmsprop,opt_adagrad]

print(solvers)
print(opts)

"""### train"""

# epochs = 20
# optimizer = 'Adam'
# optimizer2 = es.EHNewtonOptimizer()
# model.compile(optimizer=optimizer,loss='binary_crossentropy', metrics=['acc']
#               )


# hist = model.fit(x_tr_seq,y_tr,epochs=epochs,batch_size=32,validation_data=(x_val_seq,y_val),callbacks=[cb])

epochs = 90

# Train process
for solver, opt in zip(solvers, opts):
  clear_model()

  # # 1. Set Checkpoint for saving the model weights
  # # Checkpoint callback
  # checkpoint_path = f'mnist_train/model_wieghts/{solver}/cp.ckpt'
  # checkpoint_dir = os.path.dirname(checkpoint_path)

  # # Create a callback that saves the model's weights
  # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, 
  #                                                  save_weights_only=True, 
  #                                                  verbose= 1)
  # print('Model weights are saved in ', os.path.abspath(checkpoint_dir))
  # print(checkpoint_dir)

  # 2. Build a model
  new_model = tf.keras.models.clone_model(model)
  new_model.summary()

  # 3. Compile model
  new_model.compile(optimizer=opt,
                    loss='binary_crossentropy', 
                    metrics=['acc'])
  
  # 4. model fit
  hist = new_model.fit(x_tr_seq,y_tr,epochs=epochs,batch_size=32,validation_data=(x_val_seq,y_val),callbacks=[])

  # 5. save loss and acc
  for loss_and_acc in train_log_keys:
    solvers_dicts[solver][loss_and_acc] = hist.history[loss_and_acc]
    # pass


# np.save('mnist_train/train_log/solvers_dicts.npy', solvers_dicts)

def avg(arr,n):
    l = 0
    r = l+n-1
    
    arr2 = np.copy(arr)

    while r <= len(arr2)-1:
        avg = sum(arr2[l:r+1])/n
        
        for i in range(l,r+1):
            arr2[i] = avg
        # update:
        l = l + n
        r = r+ n
    
    return arr2

def avg2(arr,n):
    l = 5
    r = l+n-1
    
    arr2 = np.copy(arr)

    while r <= len(arr2)-1:
        avg = sum(arr2[l:r+1])/n
        
        for i in range(l,r+1):
            arr2[i] = avg
        # update:
        l = l + n
        r = r+ n
    
    return arr2

import matplotlib.pyplot as plt

# plot training
fig, axs = plt.subplots(2, 2, figsize=(18, 8))
fig.suptitle('IDMB Moview Review Classification, batch size= 32', fontsize=18)
epochs2 = np.linspace(1, epochs, num=epochs)
axs[0][0].plot(epochs2, solvers_dicts['Adam']['loss'], label=f'Adam')
axs[0][0].plot(epochs2, solvers_dicts['SGD']['loss'], label=f'SGD' )
axs[0][0].plot(epochs2, solvers_dicts['Newton-CG']['loss'], label=f'Newton-CG')
axs[0][0].plot(epochs2, solvers_dicts['RMSProp']['loss'], label=f'RMSprop' )
axs[0][0].plot(epochs2, solvers_dicts['AdaGrad']['loss'], label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[0][0].set_xlabel('Training Epochs', fontsize=16)
axs[0][0].set_ylabel('Training Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[0][1].plot(epochs2, solvers_dicts['Adam']['acc'], label=f'Adam')
axs[0][1].plot(epochs2, solvers_dicts['SGD']['acc'], label=f'SGD' )
axs[0][1].plot(epochs2, solvers_dicts['Newton-CG']['acc'], label=f'Newton-CG')
axs[0][1].plot(epochs2, solvers_dicts['RMSProp']['acc'], label=f'RMSprop' )
axs[0][1].plot(epochs2, solvers_dicts['AdaGrad']['acc'], label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[0][1].set_xlabel('Training Epochs', fontsize=16)
axs[0][1].set_ylabel('Training Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[0][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

axs[1][0].plot(epochs2,avg( solvers_dicts['Adam']['loss'] ,2), label=f'Adam')
axs[1][0].plot(epochs2,avg( solvers_dicts['SGD']['loss'],2), label=f'SGD' )
axs[1][0].plot(epochs2,avg( solvers_dicts['Newton-CG']['loss'],2), label=f'Newton-CG')
axs[1][0].plot(epochs2,avg( solvers_dicts['RMSProp']['loss'],2), label=f'RMSprop' )
axs[1][0].plot(epochs2,avg( solvers_dicts['AdaGrad']['loss'],2), label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[1][0].set_xlabel('Training Epochs', fontsize=16)
axs[1][0].set_ylabel('Training Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[1][1].plot(epochs2,solvers_dicts['Adam']['acc'], label=f'Adam')
axs[1][1].plot(epochs2,avg( solvers_dicts['SGD']['acc'],2), label=f'SGD' )
axs[1][1].plot(epochs2,avg( solvers_dicts['Newton-CG']['acc'],2), label=f'Newton-CG')
axs[1][1].plot(epochs2, solvers_dicts['RMSProp']['acc'], label=f'RMSprop' )
axs[1][1].plot(epochs2, solvers_dicts['AdaGrad']['acc'], label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[1][1].set_xlabel('Training Epochs', fontsize=16)
axs[1][1].set_ylabel('Training Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[1][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

fig.tight_layout()#调整整体空白
plt.subplots_adjust(wspace =0.5, hspace =0.5)#调整子图间距
fig.savefig(f'../results/rnn_model/comparison with other optimizers - 1.png', bbox_inches='tight', pad_inches=0)

import matplotlib.pyplot as plt

# plot training
fig, axs = plt.subplots(2, 2, figsize=(18, 8))
fig.suptitle('IDMB Moview Review Classification, batch size= 32', fontsize=18)
epochs2 = np.linspace(1, epochs, num=epochs)
axs[0][0].plot(epochs2, solvers_dicts['Adam']['val_loss'], label=f'Adam')
axs[0][0].plot(epochs2, solvers_dicts['SGD']['val_loss'], label=f'SGD' )
axs[0][0].plot(epochs2, solvers_dicts['Newton-CG']['val_loss'], label=f'Newton-CG')
axs[0][0].plot(epochs2, solvers_dicts['RMSProp']['val_loss'], label=f'RMSprop' )
axs[0][0].plot(epochs2, solvers_dicts['AdaGrad']['val_loss'], label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[0][0].set_xlabel('Training Epochs', fontsize=16)
axs[0][0].set_ylabel('Validation Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[0][1].plot(epochs2, solvers_dicts['Adam']['val_acc'], label=f'Adam')
axs[0][1].plot(epochs2, solvers_dicts['SGD']['val_acc'], label=f'SGD' )
axs[0][1].plot(epochs2, solvers_dicts['Newton-CG']['val_acc'], label=f'Newton-CG')
axs[0][1].plot(epochs2, solvers_dicts['RMSProp']['val_acc'], label=f'RMSprop' )
axs[0][1].plot(epochs2, solvers_dicts['AdaGrad']['val_acc'], label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[0][1].set_xlabel('Training Epochs', fontsize=16)
axs[0][1].set_ylabel('Validation Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[0][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

axs[1][0].plot(epochs2,avg( solvers_dicts['Adam']['val_loss'] ,2), label=f'Adam')
axs[1][0].plot(epochs2,avg( solvers_dicts['SGD']['val_loss'],2), label=f'SGD' )
axs[1][0].plot(epochs2,avg( solvers_dicts['Newton-CG']['val_loss'],2), label=f'Newton-CG')
axs[1][0].plot(epochs2,avg( solvers_dicts['RMSProp']['val_loss'],2), label=f'RMSprop' )
axs[1][0].plot(epochs2,avg( solvers_dicts['AdaGrad']['val_loss'],2), label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[1][0].set_xlabel('Training Epochs', fontsize=16)
axs[1][0].set_ylabel('Validation Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[1][1].plot(epochs2,avg2(solvers_dicts['Adam']['val_acc'],2), label=f'Adam')
axs[1][1].plot(epochs2,avg( solvers_dicts['SGD']['val_acc'],2), label=f'SGD' )
axs[1][1].plot(epochs2,avg( solvers_dicts['Newton-CG']['val_acc'],2), label=f'Newton-CG')
axs[1][1].plot(epochs2, avg2(solvers_dicts['RMSProp']['val_acc'],2), label=f'RMSprop' )
axs[1][1].plot(epochs2,avg2( solvers_dicts['AdaGrad']['val_acc'],2), label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[1][1].set_xlabel('Training Epochs', fontsize=16)
axs[1][1].set_ylabel('Validation Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[1][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

fig.tight_layout()#调整整体空白
plt.subplots_adjust(wspace =0.5, hspace =0.5)#调整子图间距



fig.savefig(f'../results/rnn_model/comparison with other optimizers - 2.png', bbox_inches='tight', pad_inches=0)



