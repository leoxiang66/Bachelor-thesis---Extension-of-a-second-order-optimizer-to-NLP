# -*- coding: utf-8 -*-
"""adam_pretrained.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c_n1e46E1JhGLFim_mbdVI54b1_r5-R4
"""

# %tensorflow_version 1.x

# !pip install gast==0.2.2
# !pip install -i https://test.pypi.org/simple/ newton-cg==0.0.3

# from google.colab import drive
# drive.mount('/content/drive')
# % cd /content/drive/MyDrive/pretrained_word_embedding/
# % ls

#importing libraries
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.datasets import imdb
from matplotlib import pyplot as plt
import newton_cg as es
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import *
from tensorflow.keras.layers import *
import tensorflow


#reading csv files
train = pd.read_csv('../data/Train.csv')
valid = pd.read_csv('../data/Valid.csv')             

#train_test split
x_tr, y_tr = train['text'].values, train['label'].values
x_val, y_val = valid['text'].values, valid['label'].values

import unicodedata
import re

## Step 1 and Step 2 
def preprocess_sentence(w):
    # w = unicode_to_ascii(w.lower().strip())
    w = w.lower().strip()

    # creating a space between a word and the punctuation following it
    # eg: "he is a boy." => "he is a boy ."
    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
    w = re.sub(r"([?.!,¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)

    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
    w = re.sub(r"[^a-zA-Záéíóúâêôãõàèìò?.!,¿]+", " ", w)

    w = w.strip()

    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    # w = '<start> ' + w + ' <end>'
    return w

x_tr = np.array([preprocess_sentence(i) for i in x_tr])
x_val = np.array([preprocess_sentence(i) for i in x_val])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Tokenize the sentences
tokenizer = Tokenizer()

#preparing vocabulary
tokenizer.fit_on_texts(list(x_tr))

#converting text into integer sequences
x_tr_seq  = tokenizer.texts_to_sequences(x_tr) 
x_val_seq = tokenizer.texts_to_sequences(x_val)

#padding to prepare sequences of same length
x_tr_seq  = pad_sequences(x_tr_seq, maxlen=256)
x_val_seq = pad_sequences(x_val_seq, maxlen=256)

size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding
print(size_of_vocabulary)

# y_tr, y_val = pd.get_dummies(y_tr),pd.get_dummies(y_val)
print(len(x_tr_seq), 'train sequences')
print(len(x_val_seq), 'test sequences')

print('x_train shape:', x_tr_seq.shape)
print('x_test shape:', x_val_seq.shape)

embeddings_dict = dict()
f = open('../data/cc.en.300.vec')
lines = f.readlines()

from pprint import pprint
import numpy as np
for line in lines[1:]:
    values = line.split()
    word = values[0]
    vec = np.asarray(values[1:], dtype='float32')
    embeddings_dict[word] = vec

f.close()
print('Loaded %s word vectors.' % len(embeddings_dict))
pprint(embeddings_dict['hello'])

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((size_of_vocabulary, 300))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_dict.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

tensorflow.keras.backend.clear_session() # clear previous model
vocabulary = size_of_vocabulary
word_num = 256
state_dim = 32

model = tensorflow.keras.models.Sequential()
# model.add(Embedding(vocabulary,embedding_dim,input_length=word_num))
model.add(Embedding(size_of_vocabulary,300,weights=[embedding_matrix],input_length=256,trainable=False)) 
model.add(SimpleRNN(state_dim, return_sequences=False, unroll=True))
# model.add(Bidirectional(SimpleRNN(state_dim, return_sequences=False, unroll=True)))
model.add(Dense(1,activation='sigmoid'))

model_name = 'bidirectional_rnn'

model.summary()

# Set solvers and optimizers
solvers = ['1','2']

# Set solvers_dicts for plot loss and acc
solvers_dicts ={key:{} for key in solvers}
print(solvers_dicts) 
train_log_keys = ['loss', 'acc', 'val_loss', 'val_acc']
for solver, _ in solvers_dicts.items():
  solvers_dicts[solver] = {key:{} for key in train_log_keys}
print(solvers_dicts)

# creat optimizers list
# opt_adam = Adam(learning_rate=0.0001)
# opt_sgd = SGD(learning_rate=0.001,momentum=0.9)
# opt_newton_cg = es.EHNewtonOptimizer(learning_rate=0.1, tau=10)
# opt_rmsprop = RMSprop(0.0001)
# opt_adagrad = Adagrad(0.01)
# opts = [opt_newton_cg,opt_adam, opt_sgd,opt_rmsprop,opt_adagrad]


# lrs = [0.1,0.01,0.001]
# taus=[0.1,1,10]


print(solvers)
# print(opts)

epochs=90

count = 0
lr = 0.01
tau = 1000



for i in solvers:

        opt = es.EHNewtonOptimizer(learning_rate=lr, tau=tau)
        opt_adam = Adam(0.0001)
        tensorflow.keras.backend.clear_session()

        new_model = tensorflow.keras.models.clone_model(model)
        new_model.summary()

        # 3. Compile model
        if i == '1':
          new_model.compile(optimizer=opt,
                            loss='binary_crossentropy', 
                            metrics=['accuracy'])
        
          hist = new_model.fit(x_tr_seq,y_tr,epochs=epochs,batch_size=32,validation_data=(x_val_seq,y_val),callbacks=[])


        else:
          new_model.compile(optimizer=opt_adam,
                            loss='binary_crossentropy', 
                            metrics=['accuracy'])
          
          hist1=new_model.fit(x_tr_seq,y_tr,epochs=20,batch_size=32,validation_data=(x_val_seq,y_val),callbacks=[])

          new_model.compile(optimizer=opt,
                            loss='binary_crossentropy', 
                            metrics=['accuracy'])
          hist2 = new_model.fit(x_tr_seq,y_tr,epochs=epochs-20,batch_size=32,validation_data=(x_val_seq,y_val),callbacks=[])

        # 5. save loss and acc
        for loss_and_acc in train_log_keys:
          if i =='1':
            solvers_dicts[i][loss_and_acc] = hist.history[loss_and_acc]

          else:
            solvers_dicts[i][loss_and_acc] = np.concatenate((hist1.history[loss_and_acc],hist2.history[loss_and_acc]))

def avg(arr,n):
    l = 0
    r = l+n-1
    
    arr2 = np.copy(arr)

    while r <= len(arr2)-1:
        avg = sum(arr2[l:r+1])/n
        
        for i in range(l,r+1):
            arr2[i] = avg
        # update:
        l = l + n
        r = r+ n
    
    return arr2

def avg2(arr,n):
    l = 5
    r = l+n-1
    
    arr2 = np.copy(arr)

    while r <= len(arr2)-1:
        avg = sum(arr2[l:r+1])/n
        
        for i in range(l,r+1):
            arr2[i] = avg
        # update:
        l = l + n
        r = r+ n
    
    return arr2

import matplotlib.pyplot as plt
fig, axs = plt.subplots(2, 2, figsize=(18, 8))
epochs2 = np.linspace(1, epochs, num=epochs)


for i in solvers:
        label = 'Adam pretrained' if i =='2' else 'No pretrained'

        axs[0][0].plot(epochs2, solvers_dicts[i]['loss'], label=label)
        axs[0][1].plot(epochs2, solvers_dicts[i]['acc'], label=label)
        axs[1][0].plot(epochs2, solvers_dicts[i]['val_loss'], label=label)
        axs[1][1].plot(epochs2, solvers_dicts[i]['val_acc'], label=label)


axs[0][0].set_xlabel('Training Epochs', fontsize=16)
axs[0][0].set_ylabel('Training Loss', fontsize=16)
axs[0][1].set_xlabel('Training Epochs', fontsize=16)
axs[0][1].set_ylabel('Training Accuracy', fontsize=16)
axs[1][0].set_xlabel('Training Epochs', fontsize=16)
axs[1][0].set_ylabel('Validation Loss', fontsize=16)
axs[1][1].set_xlabel('Training Epochs', fontsize=16)
axs[1][1].set_ylabel('Validation Accuracy', fontsize=16)

axs[0][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)
fig.tight_layout(rect=[0,0,1,0.9])#调整整体空白
plt.subplots_adjust(wspace =0.5, hspace =0.5)#调整子图间距



fig.savefig(f'../results/rnn_model/adam_pretrained.png', bbox_inches='tight', pad_inches=0)



