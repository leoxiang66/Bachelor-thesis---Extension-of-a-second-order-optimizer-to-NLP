# -*- coding: utf-8 -*-
"""Result(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nw2IDHNtj32wI85bc2lsCVM3Fa8W5cy8
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# !pip install gast==0.2.2
# !pip install -i https://test.pypi.org/simple/ newton-cg==0.0.3

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive')
# % cd /content/drive/MyDrive/pretrained_word_embedding/
# % ls

#importing libraries
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.datasets import imdb
from matplotlib import pyplot as plt
import newton_cg as es
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import *
from tensorflow.keras.layers import *
import tensorflow


#reading csv files
train = pd.read_csv('../data/Train.csv')
valid = pd.read_csv('../data/Valid.csv')             

#train_test split
x_tr, y_tr = train['text'].values, train['label'].values
x_val, y_val = valid['text'].values, valid['label'].values

import unicodedata
import re

## Step 1 and Step 2 
def preprocess_sentence(w):
    # w = unicode_to_ascii(w.lower().strip())
    w = w.lower().strip()

    # creating a space between a word and the punctuation following it
    # eg: "he is a boy." => "he is a boy ."
    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
    w = re.sub(r"([?.!,¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)

    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
    w = re.sub(r"[^a-zA-Záéíóúâêôãõàèìò?.!,¿]+", " ", w)

    w = w.strip()

    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    # w = '<start> ' + w + ' <end>'
    return w

x_tr = np.array([preprocess_sentence(i) for i in x_tr])
x_val = np.array([preprocess_sentence(i) for i in x_val])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Tokenize the sentences
tokenizer = Tokenizer()

#preparing vocabulary
tokenizer.fit_on_texts(list(x_tr))

#converting text into integer sequences
x_tr_seq  = tokenizer.texts_to_sequences(x_tr) 
x_val_seq = tokenizer.texts_to_sequences(x_val)

#padding to prepare sequences of same length
x_tr_seq  = pad_sequences(x_tr_seq, maxlen=256)
x_val_seq = pad_sequences(x_val_seq, maxlen=256)

size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding
print(size_of_vocabulary)

y_tr, y_val = pd.get_dummies(y_tr),pd.get_dummies(y_val)
print(len(x_tr_seq), 'train sequences')
print(len(x_val_seq), 'test sequences')

print('x_train shape:', x_tr_seq.shape)
print('x_test shape:', x_val_seq.shape)

embeddings_dict = dict()
f = open('../data/cc.en.300.vec')
lines = f.readlines()

from pprint import pprint
import numpy as np
for line in lines[1:]:
    values = line.split()
    word = values[0]
    vec = np.asarray(values[1:], dtype='float32')
    embeddings_dict[word] = vec

f.close()
print('Loaded %s word vectors.' % len(embeddings_dict))
pprint(embeddings_dict['hello'])

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((size_of_vocabulary, 300))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_dict.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

class Self_Attention(Layer):

    def __init__(self, output_dim, **kwargs):
        self.output_dim = output_dim
        super(Self_Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        # 为该层创建一个可训练的权重
        #inputs.shape = (batch_size, time_steps, seq_len)
        self.kernel = self.add_weight(name='kernel',
                                      shape=(3,input_shape[2], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)

        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它

    def call(self, x):
        WQ = K.dot(x, self.kernel[0])
        WK = K.dot(x, self.kernel[1])
        WV = K.dot(x, self.kernel[2])

        print("WQ.shape",WQ.shape)

        print("K.permute_dimensions(WK, [0, 2, 1]).shape",K.permute_dimensions(WK, [0, 2, 1]).shape)


        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))

        QK = QK / (self.output_dim**0.5)  

        QK = K.softmax(QK)

        print("QK.shape",QK.shape)

        V = K.batch_dot(QK,WV)

        return V

    def compute_output_shape(self, input_shape):

        return (input_shape[0],input_shape[1],self.output_dim)


    def get_config(self):
        config=super(Self_Attention,self).get_config()
        config.update({
            "output_dim":self.output_dim,
        })
        return config

max_features = 91064
maxlen = 256
batch_size = 32

S_inputs = Input(shape=(maxlen,), dtype='int32')

# embeddings = Embedding(max_features, 128)(S_inputs)
embeddings = Embedding(max_features,300,weights=[embedding_matrix],input_length=256,trainable=False)(S_inputs)


O_seq = Self_Attention(128)(embeddings)


O_seq = GlobalAveragePooling1D()(O_seq)

O_seq = Dropout(0.5)(O_seq)

outputs = Dense(2, activation='softmax')(O_seq)


model = Model(inputs=S_inputs, outputs=outputs)

print(model.summary())

# Set solvers and optimizers
solvers = ['Newton-CG','Adam', 'SGD','RMSProp','AdaGrad']

# Set solvers_dicts for plot loss and acc
solvers_dicts ={key:{} for key in solvers}
print(solvers_dicts) 
train_log_keys = ['loss', 'acc', 'val_loss', 'val_acc']
for solver, _ in solvers_dicts.items():
  solvers_dicts[solver] = {key:{} for key in train_log_keys}
print(solvers_dicts)

# creat optimizers list
opt_adam = Adam(learning_rate=0.0001)
opt_sgd = SGD(learning_rate=0.001,momentum=0.9)
opt_newton_cg = es.EHNewtonOptimizer(learning_rate=0.1, tau=10)
opt_rmsprop = RMSprop(0.0001)
opt_adagrad = Adagrad(0.01)
opts = [opt_newton_cg,opt_adam, opt_sgd,opt_rmsprop,opt_adagrad]

print(solvers)
print(opts)

epochs= 100

for solver, opt in zip(solvers, opts):
  tensorflow.keras.backend.clear_session()

  new_model = tensorflow.keras.models.clone_model(model)
  new_model.summary()

  # 3. Compile model
  new_model.compile(optimizer=opt,
                    loss='categorical_crossentropy', 
                    metrics=['accuracy'])
  
  hist = new_model.fit(x_tr_seq,y_tr,epochs=epochs,batch_size=32,validation_data=(x_val_seq,y_val),callbacks=[])

  # 5. save loss and acc
  for loss_and_acc in train_log_keys:
    solvers_dicts[solver][loss_and_acc] = hist.history[loss_and_acc]

def avg(arr,n):
    l = 0
    r = l+n-1
    
    arr2 = np.copy(arr)

    while r <= len(arr2)-1:
        avg = sum(arr2[l:r+1])/n
        
        for i in range(l,r+1):
            arr2[i] = avg
        # update:
        l = l + n
        r = r+ n
    
    return arr2

def avg2(arr,n):
    l = 5
    r = l+n-1
    
    arr2 = np.copy(arr)

    while r <= len(arr2)-1:
        avg = sum(arr2[l:r+1])/n
        
        for i in range(l,r+1):
            arr2[i] = avg
        # update:
        l = l + n
        r = r+ n
    
    return arr2

import matplotlib.pyplot as plt

# plot training
fig, axs = plt.subplots(1, 2, figsize=(18, 8))
fig.suptitle('IDMB Movie Review Classification, batch size= 32', fontsize=18)
epochs2 = np.linspace(1, epochs, num=epochs)
axs[0].plot(epochs2, solvers_dicts['Adam']['loss'], label=f'Adam')
axs[0].plot(epochs2, solvers_dicts['SGD']['loss'], label=f'SGD' )
axs[0].plot(epochs2, solvers_dicts['Newton-CG']['loss'], label=f'Newton-CG')
axs[0].plot(epochs2, solvers_dicts['RMSProp']['loss'], label=f'RMSprop' )
axs[0].plot(epochs2, solvers_dicts['AdaGrad']['loss'], label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[0].set_xlabel('Training Epochs', fontsize=16)
axs[0].set_ylabel('Training Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[1].plot(epochs2, solvers_dicts['Adam']['acc'], label=f'Adam')
axs[1].plot(epochs2, solvers_dicts['SGD']['acc'], label=f'SGD' )
axs[1].plot(epochs2, solvers_dicts['Newton-CG']['acc'], label=f'Newton-CG')
axs[1].plot(epochs2, solvers_dicts['RMSProp']['acc'], label=f'RMSprop' )
axs[1].plot(epochs2, solvers_dicts['AdaGrad']['acc'], label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[1].set_xlabel('Training Epochs', fontsize=16)
axs[1].set_ylabel('Training Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

fig.tight_layout(rect=[0,0,1,0.9])#调整整体空白
plt.subplots_adjust(wspace =0.5, hspace =0.5)#调整子图间距
fig.savefig(f'/home/ge25pof/thesis_work/results/attention_model/comparison with other optimizers - 1.png', bbox_inches='tight', pad_inches=0)

import matplotlib.pyplot as plt

# plot training
fig, axs = plt.subplots(2, 2, figsize=(18, 8))
fig.suptitle('IDMB Movie Review Classification, batch size= 32', fontsize=18)
epochs2 = np.linspace(1, epochs, num=epochs)
axs[0][0].plot(epochs2, solvers_dicts['Adam']['val_loss'], label=f'Adam')
axs[0][0].plot(epochs2, solvers_dicts['SGD']['val_loss'], label=f'SGD' )
axs[0][0].plot(epochs2, solvers_dicts['Newton-CG']['val_loss'], label=f'Newton-CG')
axs[0][0].plot(epochs2, solvers_dicts['RMSProp']['val_loss'], label=f'RMSprop' )
axs[0][0].plot(epochs2, solvers_dicts['AdaGrad']['val_loss'], label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[0][0].set_xlabel('Training Epochs', fontsize=16)
axs[0][0].set_ylabel('Validation Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[0][1].plot(epochs2, solvers_dicts['Adam']['val_acc'], label=f'Adam')
axs[0][1].plot(epochs2, solvers_dicts['SGD']['val_acc'], label=f'SGD' )
axs[0][1].plot(epochs2, solvers_dicts['Newton-CG']['val_acc'], label=f'Newton-CG')
axs[0][1].plot(epochs2, solvers_dicts['RMSProp']['val_acc'], label=f'RMSprop' )
axs[0][1].plot(epochs2, solvers_dicts['AdaGrad']['val_acc'], label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[0][1].set_xlabel('Training Epochs', fontsize=16)
axs[0][1].set_ylabel('Validation Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[0][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

axs[1][0].plot(epochs2,( solvers_dicts['Adam']['val_loss'] ), label=f'Adam')
axs[1][0].plot(epochs2,( solvers_dicts['SGD']['val_loss']), label=f'SGD' )
axs[1][0].plot(epochs2,( solvers_dicts['Newton-CG']['val_loss']), label=f'Newton-CG')
axs[1][0].plot(epochs2,( solvers_dicts['RMSProp']['val_loss']), label=f'RMSprop' )
axs[1][0].plot(epochs2,( solvers_dicts['AdaGrad']['val_loss']), label=f'AdaGrad' )

# axs[0].set_xlim(0, 19)
# axs[0].set_xticks(epochs)

axs[1][0].set_xlabel('Training Epochs', fontsize=16)
axs[1][0].set_ylabel('Validation Loss', fontsize=16)
# axs[0].legend(loc="upper right", fontsize=10)

axs[1][1].plot(epochs2,(solvers_dicts['Adam']['val_acc']), label=f'Adam')
axs[1][1].plot(epochs2,avg( solvers_dicts['SGD']['val_acc'],2), label=f'SGD' )
axs[1][1].plot(epochs2,avg( solvers_dicts['Newton-CG']['val_acc'],2), label=f'Newton-CG')
axs[1][1].plot(epochs2, (solvers_dicts['RMSProp']['val_acc']), label=f'RMSprop' )
axs[1][1].plot(epochs2,avg2( solvers_dicts['AdaGrad']['val_acc'],2), label=f'AdaGrad' )
# axs[1].set_xticks(epochs2)
axs[1][1].set_xlabel('Training Epochs', fontsize=16)
axs[1][1].set_ylabel('Validation Accuracy', fontsize=16)
# axs[1].legend(loc="upper left", fontsize=10)
axs[1][1].legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.,fontsize=16)

fig.tight_layout(rect=[0,0,1,0.9])#调整整体空白
plt.subplots_adjust(wspace =0.5, hspace =0.5)#调整子图间距



fig.savefig(f'../results/attention_model/comparison with other optimizers - 2.png', bbox_inches='tight', pad_inches=0)
